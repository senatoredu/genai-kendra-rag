{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "743c6d48-a97a-4cb4-8ea2-3c367b656c3c",
   "metadata": {},
   "source": [
    "# Building Conversational GenAI Chatbots for Enterprises - Workshop\n",
    "---\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bade8627-462c-4523-9d6f-3a7fe80841d3",
   "metadata": {},
   "source": [
    "This notebook is based on this aws blog:\n",
    "* https://aws.amazon.com/blogs/machine-learning/quickly-build-high-accuracy-generative-ai-applications-on-enterprise-data-using-amazon-kendra-langchain-and-large-language-models/\n",
    "\n",
    "And this repo:\n",
    "* https://github.com/aws-samples/amazon-kendra-langchain-extensions\n",
    "\n",
    "---\n",
    "\n",
    "### Solution overview\n",
    "The following diagram shows the architecture of a GenAI application with a RAG approach.\n",
    "\n",
    "<img src=\"https://d2908q01vomqb2.cloudfront.net/f1f836cb4ea6efb2a0b1b99f41ad8b103eff4b59/2023/05/02/ML-13807-image001-new.png\">\n",
    "\n",
    "We use an Amazon Kendra index to ingest enterprise unstructured data from data sources such as wiki pages, MS SharePoint sites, Atlassian Confluence, and document repositories such as Amazon S3. When a user interacts with the GenAI app, the flow is as follows:\n",
    "\n",
    "1. The user makes a request to the GenAI app.\n",
    "2. The app issues a search query to the Amazon Kendra index based on the user request.\n",
    "3. The index returns search results with excerpts of relevant documents from the ingested enterprise data.\n",
    "4. The app sends the user request and along with the data retrieved from the index as context in the LLM prompt.\n",
    "5. The LLM returns a succinct response to the user request based on the retrieved data.\n",
    "6. The response from the LLM is sent back to the user.\n",
    "\n",
    "With this architecture, you can choose the most suitable LLM for your use case. LLM options include our partners Meta, Hugging Face, AI21 Labs, Cohere, and others hosted on an Amazon SageMaker endpoint, as well as models by companies like Anthropic and OpenAI. With Amazon Bedrock, you will be able to choose Amazon Titan, Amazon’s own LLM, or partner LLMs such as those from AI21 Labs and Anthropic with APIs securely without the need for your data to leave the AWS ecosystem. The additional benefits that Amazon Bedrock will offer include a serverless architecture, a single API to call the supported LLMs, and a managed service to streamline the developer workflow.\n",
    "\n",
    "For the best results, a GenAI app needs to engineer the prompt based on the user request and the specific LLM being used. Conversational AI apps also need to manage the chat history and the context. GenAI app developers can use open-source frameworks such as LangChain that provide modules to integrate with the LLM of choice, and orchestration tools for activities such as chat history management and prompt engineering. We have provided the KendraIndexRetriever class, which implements a LangChain retriever interface, which applications can use in conjunction with other LangChain interfaces such as chains to retrieve data from an Amazon Kendra index. We have also provided a few sample applications in the GitHub repo. You can deploy this solution in your AWS account using the step-by-step guide in this post."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a935567-2d68-4ba5-887e-a8e15fbad714",
   "metadata": {
    "tags": []
   },
   "source": [
    "---\n",
    "\n",
    "0. [Prerequisites](#Prerequisites)\n",
    "1. [Permissions and environment variables](#1.-Permissions-and-environment-variables)\n",
    "2. [Select a pre-trained model](#2.-Select-a-pre-trained-model)\n",
    "3. [Retrieve Artifacts & Deploy an Endpoint](#3.-Retrieve-Artifacts-&-Deploy-an-Endpoint)\n",
    "4. [Query endpoint and parse response](#4.-Query-endpoint-and-parse-response)\n",
    "5. [Query endpoint with Langchain and Kendra Index](#5.-Query-endpoint-with-Langchain-and-Kendra-Index)\n",
    "6. [[OPTIONAL] Installing Streamlet application and running a WebUI for a chatbot](#6.-[OPTIONAL]-Installing-Streamlit-application-and-running-a-WebUI-for-a-chatbot)\n",
    "7. [Clean up the endpoint](#7.-Clean-up-the-endpoint)\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0a93461-d37f-41f4-aa1f-47bd04016865",
   "metadata": {},
   "source": [
    "# Prerequisites\n",
    "\n",
    "# Installation and import of required dependencies, further setup tasks\n",
    "\n",
    "For this lab, we will use the following libraries:\n",
    "\n",
    " - SageMaker SDK for interacting with Amazon SageMaker\n",
    " - boto3, the AWS SDK for python\n",
    " - os, a python library implementing miscellaneous operating system interfaces \n",
    " - tarfile, a python library to read and write tar archive files\n",
    " - io, native Python library, provides Python’s main facilities for dealing with various types of I/O.\n",
    " - tqdm, a utility to easily show a smart progress meter for synchronous operations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8520390f-9909-4aba-b5e8-cc92e8c6fc21",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import sagemaker\n",
    "import boto3\n",
    "import os\n",
    "import tarfile\n",
    "import requests\n",
    "from io import BytesIO\n",
    "from tqdm import tqdm\n",
    "\n",
    "## if you get an error message about the NumPy and SciPy version you can safely ignore it and move on"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13f94230-975c-428c-ba4b-635f45172e8b",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Setup of notebook environment\n",
    "\n",
    "Before we begin with the actual work, we need to setup the notebook environment respectively. This includes:\n",
    "\n",
    "- retrieval of the execution role our SageMaker Studio domain is associated with for later usage\n",
    "- retrieval of our account_id for later usage\n",
    "- retrieval of the chosen region for later usage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fb17d15-59cd-4232-b42f-b15dc9ac485b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Retrieve SM execution role\n",
    "aws_role = sagemaker.get_execution_role()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bc9333b-2b74-4e0c-a5ce-4ad86ebd6cf7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Create a new STS client\n",
    "sts_client = boto3.client('sts')\n",
    "\n",
    "# Call the GetCallerIdentity operation to retrieve the account ID\n",
    "response = sts_client.get_caller_identity()\n",
    "account_id = response['Account']\n",
    "account_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "496febd5-bdd0-4d8c-9bd9-bfc24161daca",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Retrieve region\n",
    "aws_region = boto3.Session().region_name\n",
    "aws_region"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7d1f3d0-810d-4ee9-b525-3e6ed238ed36",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Setup of S3 bucket for Amazon Kendra storage of knowledge documents\n",
    "\n",
    "Amazon Kendra provides multiple built-in adapters for integrating with data sources to build up a document index, e.g. S3, Microsoft SharePoint sites, Atlassian Confluence, web-scraper, RDS, Box, Dropbox, ... . In this lab we will store the documents containing the Enterprise data knowledge to be infused into the application in S3. For this purpose we will create a dedicated S3 bucket."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7e84b78-7453-4045-a7f9-5d1a2d42b9ec",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Uploading knowledge documents into an Amazon Kendra index\n",
    "\n",
    "Next we are going to add some more documents from S3 to show how easy it is to integrate different data sources to a Kendra Index. \n",
    "First we are going to download some interesting pdf files from the internet, but please feel free to drop any pdf you might find interesting in it as well. \n",
    "In our case we will add some financial document relating to Amazon's yearly performance, also some client document for banking, and lastly some AWS Documentation docs.\n",
    "\n",
    "Important: Before you proceed to the create the bucket in the next step, delete the Kendra Data Source named 'genAI_conf_Kendra_Data_Source' (this is provisioned as part of the Studio Engine but is used in a different workshop context and not relevant for ours). \n",
    "\n",
    "To Delete it:\n",
    "\n",
    "1) Navigate to the Kendra console: https://us-east-1.console.aws.amazon.com/kendra/home?region=us-east-1#indexes\n",
    "2) Click on the Only Kendra Index (starts with \"genai-conference-index-xxxxx\") \n",
    "3) On the left side of the console, under 'Data Management', select 'Data sources' \n",
    "4) Select the only Data source (named 'genAI_conf_Kendra_Data_Source') \n",
    "5) Select 'Actions' and click 'Delete' to delete this Data source (type 'Delete' in box to confirm)\n",
    "6) You'd get a notification that the Data Source is being deleted, please proceed with the next step while this is happening in the background.\n",
    "\n",
    "<img src=\"https://github.com/senatoredu/genai-kendra-rag/blob/main/datasourcedelete.png?raw=true\">\n",
    "\n",
    "Proceed to run the next cell to create your S3 bucket to store the Files and later create your own S3 Data Connector pointing to this bucket."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c571ede-db5c-4ae7-b043-b9550ca4ccd7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import boto3\n",
    "import requests\n",
    "from io import BytesIO\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Create an S3 client\n",
    "s3 = boto3.client('s3')\n",
    "\n",
    "# Create a bucket if it doesn't exist\n",
    "bucket_name = f'immersion-day-bucket-{account_id}-{aws_region}'\n",
    "if s3.list_buckets()['Buckets']:\n",
    "    for bucket in s3.list_buckets()['Buckets']:\n",
    "        if bucket['Name'] == bucket_name:\n",
    "            break\n",
    "    else:\n",
    "        s3.create_bucket(Bucket=bucket_name)\n",
    "else:\n",
    "    s3.create_bucket(Bucket=bucket_name)\n",
    "\n",
    "# List of URLs to download PDFs from\n",
    "pdf_urls = [\n",
    "    \"https://s2.q4cdn.com/299287126/files/doc_financials/2023/ar/Amazon-2022-Annual-Report.pdf\",\n",
    "    \"https://s2.q4cdn.com/299287126/files/doc_financials/2022/ar/Amazon-2021-Annual-Report.pdf\",\n",
    "    \"https://s2.q4cdn.com/299287126/files/doc_financials/2021/ar/Amazon-2020-Annual-Report.pdf\",\n",
    "    \"https://s2.q4cdn.com/299287126/files/doc_financials/2020/ar/2019-Annual-Report.pdf\", \n",
    "    \"https://s2.q4cdn.com/299287126/files/doc_financials/annual/2018-Annual-Report.pdf\",\n",
    "    \"https://docs.aws.amazon.com/pdfs/whitepapers/latest/microservices-on-aws/microservices-on-aws.pdf\",\n",
    "    \"https://docs.aws.amazon.com/pdfs/whitepapers/latest/overview-aws-cloud-adoption-framework/overview-aws-cloud-adoption-framework.pdf\",\n",
    "    \"https://docs.aws.amazon.com/pdfs/whitepapers/latest/aws-multi-region-fundamentals/aws-multi-region-fundamentals.pdf\",\n",
    "    \"https://docs.aws.amazon.com/pdfs/whitepapers/latest/aws-overview/aws-overview.pdf\",\n",
    "    \"https://docs.aws.amazon.com/pdfs/whitepapers/latest/docker-on-aws/docker-on-aws.pdf\", \n",
    "    \"https://docs.aws.amazon.com/pdfs/whitepapers/latest/overview-deployment-options/overview-deployment-options.pdf\",\n",
    "    \"https://www.nab.com.au/content/dam/nabrwd/documents/terms-and-conditions/banking/nab-personal-transactions-savings-terms-and-conditions.pdf\",\n",
    "    \"https://www.nab.com.au/content/dam/nabrwd/documents/terms-and-conditions/banking/personal-transaction-and-savings-offsale.pdf\",\n",
    "    \"https://budget.gov.au/content/bp1/download/bp1_2023-24_230727.pdf\", \n",
    "    \"https://budget.gov.au/content/overview/download/budget_overview-20230511.pdf\",\n",
    "    \"https://budget.gov.au/content/bp2/download/bp2_2023-24.pdf\",\n",
    "    \"https://archive.budget.gov.au/2022-23/bp2/download/bp2_2022-23.pdf\",\n",
    "]\n",
    "\n",
    "# Download PDFs from the URLs and upload them to the S3 bucket\n",
    "for url in tqdm(pdf_urls):\n",
    "    response = requests.get(url, stream=True)\n",
    "    filename = os.path.basename(url)\n",
    "    print(f\"Working on {filename}\")\n",
    "    fileobj = BytesIO()\n",
    "    total_size = int(response.headers.get('content-length', 0))\n",
    "    block_size = 1024\n",
    "    progress_bar = tqdm(total=total_size, unit='iB', unit_scale=True)\n",
    "    for data in response.iter_content(block_size):\n",
    "        progress_bar.update(len(data))\n",
    "        fileobj.write(data)\n",
    "    progress_bar.close()\n",
    "    fileobj.seek(0)\n",
    "    s3.upload_fileobj(fileobj, bucket_name, filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "568dc2f6-6cba-4b4d-b600-b46997ac545b",
   "metadata": {},
   "source": [
    "Lets use those documents in Kendra. First navigate to the Kendra console. \n",
    "\n",
    "Under \"Data Management\" you will find the tab \"Data Sources\". Navigate there and add a new data source via \"Add data source\". \n",
    "Take some time to inspect all the different connectors that are there for you to use out of the box. We will use s3 as our source. \n",
    "\n",
    "It is worth noting that Kendra respect enterprise level access attributes. That means, that it can deny queries if a user is not authorized to retrieve a document. \n",
    "\n",
    "The animation below shows how to add an s3 data source to kendra to index. We are creating a new IAM role as well as setting the indexing frequncy to \"on-demand\". \n",
    "\n",
    "<p align=\"center\">\n",
    "  <img src=\"https://raw.githubusercontent.com/aws-samples/generative-ai-on-aws-immersion-day/4727dd546aa15eeef0440aa53a39ecf85aa49e17/img/new_s3_connection.gif\" alt=\"How to add a kendra s3 data source \"/>\n",
    "</p>\n",
    "\n",
    "\n",
    "After the connection has been established, you can sync your data source by clicking \"sync now\". "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76ba1a07-a41f-4d7a-beb7-2f8b82facd8c",
   "metadata": {
    "tags": []
   },
   "source": [
    "### 1. Permissions and environment variables for SageMaker\n",
    "\n",
    "---\n",
    "To host the LLaMA model on Amazon SageMaker, we need to set up and authenticate the use of AWS services. Here, we use the execution role associated with the current notebook as the AWS account role with SageMaker access. \n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0221f42-b4de-4c97-be49-c95915a2b945",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import sagemaker, boto3, json\n",
    "from sagemaker.session import Session\n",
    "\n",
    "sagemaker_session = Session()\n",
    "aws_role = sagemaker_session.get_caller_identity_arn()\n",
    "aws_region = boto3.Session().region_name\n",
    "sess = sagemaker.Session()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9addf0b8-479f-4f7d-b420-d53e59e6480f",
   "metadata": {
    "tags": []
   },
   "source": [
    "### 2. Select a pre-trained model\n",
    "***\n",
    "You can continue with the default model, or can choose a different model. A complete list of SageMaker pre-trained models can also be accessed at [SageMaker pre-trained Models](https://sagemaker.readthedocs.io/en/stable/doc_utils/pretrainedmodels.html#). Be sure to select a model that can be used for text2text generation.\n",
    "\n",
    "For our workshop we will use Llama 7b, which is a Llama 2 model from Meta that contains 7 billion parameters and is optimized for dialogue and assistant-like use cases. Llama 2 was pre-trained on 2 trillion tokens of data from publicly available sources. \n",
    "More details about its use and technical details here: \n",
    "\n",
    "https://aws.amazon.com/blogs/machine-learning/llama-2-foundation-models-from-meta-are-now-available-in-amazon-sagemaker-jumpstart/\n",
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17ed3d4d-5c77-42d3-bf1d-dccc6be203b0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model_id, model_version = \"meta-textgeneration-llama-2-7b-f\", \"*\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "102b0b66-a447-4a71-9308-8f0bd3795334",
   "metadata": {},
   "source": [
    "### 3. Retrieve Artifacts & Deploy an Endpoint\n",
    "\n",
    "***\n",
    "\n",
    "Using SageMaker, we can perform inference on the pre-trained model, even without fine-tuning it first on a new dataset. We start by retrieving the `deploy_image_uri`, `deploy_source_uri`, and `model_uri` for the pre-trained model. To host the pre-trained model, we create an instance of [`sagemaker.model.Model`](https://sagemaker.readthedocs.io/en/stable/api/inference/model.html) and deploy it. This may take a few minutes.\n",
    "\n",
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8bd9ef9-2bce-4da1-8ee1-51026ba9d52f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sagemaker.jumpstart.model import JumpStartModel\n",
    "my_model = JumpStartModel(model_id = \"meta-textgeneration-llama-2-7b-f\")\n",
    "predictor = my_model.deploy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be5c20f1-9360-4637-9649-dede0feaabc8",
   "metadata": {},
   "source": [
    "Once SageMaker is done deploying the model on an endpoint you'd see a '----------------!' output. \n",
    "You can also see the SageMaker model and endpoint by navigating to the 'SageMaker Dashboard' tab on the SageMaker console."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b21f7e4-cbf5-4b37-9648-82b6373d4df5",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### 6. Installing Streamlit application and running a WebUI for a chatbot\n",
    "\n",
    "---\n",
    "This sections provides instructions on how to run a streamlit application within sagemaker studio and accessing it using jupyter proxy. The commands and instructions below need to be run inside a **SageMaker System Terminal**. \n",
    "\n",
    "First we will download a repo that contains a set of samples to work with Langchain and Amazon Kendra. It currently has samples for working with a Kendra retriever class to execute a QA chain for Meta, Open AI and Anthropic providers. From the repo we will install all the required modules and dependencies we need.\n",
    "\n",
    "---\n",
    "\n",
    "1. Launch a new SageMaker System Terminal \n",
    "   1. From the SageMaker Studio Home screen select `Open Launcher`\n",
    "   2. From the Launcher panel under `Utilities and files` select `System terminal`\n",
    "   \n",
    "Clone the repository\n",
    "```bash\n",
    "git clone https://github.com/senatoredu/amazon-kendra-langchain-extensions.git\n",
    "```\n",
    "\n",
    "Move to the repo dir\n",
    "```bash\n",
    "cd amazon-kendra-langchain-extensions\n",
    "```\n",
    "\n",
    "Move to the samples dir\n",
    "```bash\n",
    "cd kendra_retriever_samples\n",
    "```\n",
    "\n",
    "Install the dependencies using pip (if you get a pip dependency resolver error at end safely ignore)\n",
    "\n",
    "\n",
    "```bash\n",
    "pip install -r requirements.txt\n",
    "```\n",
    "\n",
    "\n",
    "2. Activate the conda environment\n",
    "```\n",
    "conda activate studio\n",
    "```\n",
    "\n",
    "3. Set your environment variables. See image below if you need help finding the ID and Endpoint.\n",
    "```\n",
    "export AWS_REGION=us-east-1\n",
    "export KENDRA_INDEX_ID=\"<YOUR_KENDRA_INDEX_ID>\"\n",
    "export LLAMA_2_ENDPOINT=\"<YOUR_SAGEMAKER_ENDPOINT_FOR_LLAMA_2>\"\n",
    "\n",
    "```\n",
    "<p align=\"center\">\n",
    "    <img src=\"https://github.com/senatoredu/genai-kendra-rag/blob/main/kendraindex.png?raw=true\"> \n",
    "</p> \n",
    "\n",
    "<p align=\"center\">\n",
    "    <img src=\"https://github.com/senatoredu/genai-kendra-rag/blob/main/sagemakerendpoint.png?raw=true\">\n",
    "</p> \n",
    "\n",
    "<p align=\"center\">\n",
    "    <img src=\"https://github.com/senatoredu/genai-kendra-rag/blob/main/sagemaker_endpointname.png?raw=true\">\n",
    "</p> \n",
    "\n",
    "\n",
    "4. Run the streamlit application\n",
    "```\n",
    "cd /home/sagemaker-user/amazon-kendra-langchain-extensions/kendra_retriever_samples/\n",
    "\n",
    "streamlit run app.py llama2\n",
    "```\n",
    "5. This will output something similar to the below.\n",
    "```\n",
    "Collecting usage statistics. To deactivate, set browser.gatherUsageStats to False.\n",
    "\n",
    "\n",
    "  You can now view your Streamlit app in your browser.\n",
    "\n",
    "  Network URL: http://169.255.255.2:8501\n",
    "  External URL: http://18.213.200.192:8501\n",
    "```\n",
    "6. Copy the current URL of the SageMaker Studio which should have the form (you can copy the URL from your current browser tab of sagemaker studio):\n",
    "```\n",
    "https://<YOUR_STUDIO_DOMAIN>.studio.<AWS_REGION>.sagemaker.aws/jupyter/default/lab/workspaces/auto-Z/tree/kendra_rag_demo.ipynb\n",
    "```\n",
    "7. Delete everything from `lab/` onwards and replace it with `proxy/<PORT>/`\n",
    "   1. DON'T FORGET THE END `/`\n",
    "```\n",
    "https://<YOUR_STUDIO_DOMAIN>.studio.<AWS_REGION>.sagemaker.aws/jupyter/default/proxy/8501/\n",
    "```\n",
    "8. Paste the new address into the browser and you will now be able to access your chatbot UI which uses Langchain and Kendra. Each response will list the sources from Kendra it used for its answers."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26d37967-38b0-47b2-b36a-1938a1a239d3",
   "metadata": {},
   "source": [
    "### 7. Clean up the endpoint\n",
    "---\n",
    "When you're done with the lab and if you'd like to delete the SageMaker endpoint run the cell below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3eb3d629-4778-4b72-b65e-2597ffa21805",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Delete the SageMaker endpoint\n",
    "model_predictor.delete_model()\n",
    "model_predictor.delete_endpoint()"
   ]
  }
 ],
 "metadata": {
  "availableInstances": [
   {
    "_defaultOrder": 0,
    "_isFastLaunch": true,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 4,
    "name": "ml.t3.medium",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 1,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.t3.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 2,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.t3.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 3,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.t3.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 4,
    "_isFastLaunch": true,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.m5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 5,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.m5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 6,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.m5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 7,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.m5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 8,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.m5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 9,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.m5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 10,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.m5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 11,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.m5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 12,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.m5d.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 13,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.m5d.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 14,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.m5d.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 15,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.m5d.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 16,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.m5d.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 17,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.m5d.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 18,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.m5d.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 19,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.m5d.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 20,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": true,
    "memoryGiB": 0,
    "name": "ml.geospatial.interactive",
    "supportedImageNames": [
     "sagemaker-geospatial-v1-0"
    ],
    "vcpuNum": 0
   },
   {
    "_defaultOrder": 21,
    "_isFastLaunch": true,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 4,
    "name": "ml.c5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 22,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.c5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 23,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.c5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 24,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.c5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 25,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 72,
    "name": "ml.c5.9xlarge",
    "vcpuNum": 36
   },
   {
    "_defaultOrder": 26,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 96,
    "name": "ml.c5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 27,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 144,
    "name": "ml.c5.18xlarge",
    "vcpuNum": 72
   },
   {
    "_defaultOrder": 28,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.c5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 29,
    "_isFastLaunch": true,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.g4dn.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 30,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.g4dn.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 31,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.g4dn.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 32,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.g4dn.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 33,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.g4dn.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 34,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.g4dn.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 35,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 61,
    "name": "ml.p3.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 36,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 244,
    "name": "ml.p3.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 37,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 488,
    "name": "ml.p3.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 38,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.p3dn.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 39,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.r5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 40,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.r5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 41,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.r5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 42,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.r5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 43,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.r5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 44,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.r5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 45,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 512,
    "name": "ml.r5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 46,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.r5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 47,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.g5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 48,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.g5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 49,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.g5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 50,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.g5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 51,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.g5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 52,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.g5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 53,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.g5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 54,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.g5.48xlarge",
    "vcpuNum": 192
   },
   {
    "_defaultOrder": 55,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 1152,
    "name": "ml.p4d.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 56,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 1152,
    "name": "ml.p4de.24xlarge",
    "vcpuNum": 96
   }
  ],
  "instance_type": "ml.t3.medium",
  "kernelspec": {
   "display_name": "Python 3 (Data Science 3.0)",
   "language": "python",
   "name": "python3__SAGEMAKER_INTERNAL__arn:aws:sagemaker:us-east-1:081325390199:image/sagemaker-data-science-310-v1"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
