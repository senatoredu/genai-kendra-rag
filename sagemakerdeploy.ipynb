{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "743c6d48-a97a-4cb4-8ea2-3c367b656c3c",
   "metadata": {},
   "source": [
    "# Building Conversational GenAI Chatbots for Enterprises\n",
    "\n",
    "#### Image & Kernel Type: on the top right corner select Image: Data Science 3.0, and Kernel: Python 3, leave rest as default\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bade8627-462c-4523-9d6f-3a7fe80841d3",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "\n",
    "This notebook is based on this aws blog:\n",
    "* https://aws.amazon.com/blogs/machine-learning/quickly-build-high-accuracy-generative-ai-applications-on-enterprise-data-using-amazon-kendra-langchain-and-large-language-models/\n",
    "\n",
    "And this repo:\n",
    "* https://github.com/aws-samples/amazon-kendra-langchain-extensions\n",
    "\n",
    "---\n",
    "\n",
    "### Solution overview\n",
    "The following diagram shows the architecture of a GenAI application with a RAG approach.\n",
    "\n",
    "<img src=\"https://d2908q01vomqb2.cloudfront.net/f1f836cb4ea6efb2a0b1b99f41ad8b103eff4b59/2023/05/02/ML-13807-image001-new.png\">\n",
    "\n",
    "We use an Amazon Kendra index to ingest enterprise unstructured data from data sources such as wiki pages, MS SharePoint sites, Atlassian Confluence, and document repositories such as Amazon S3. When a user interacts with the GenAI app, the flow is as follows:\n",
    "\n",
    "1. The user makes a request to the GenAI app.\n",
    "2. The app issues a search query to the Amazon Kendra index based on the user request.\n",
    "3. The index returns search results with excerpts of relevant documents from the ingested enterprise data.\n",
    "4. The app sends the user request and along with the data retrieved from the index as context in the LLM prompt.\n",
    "5. The LLM returns a succinct response to the user request based on the retrieved data.\n",
    "6. The response from the LLM is sent back to the user.\n",
    "\n",
    "With this architecture, you can choose the most suitable LLM for your use case. LLM options include our partners Hugging Face, AI21 Labs, Cohere, and others hosted on an Amazon SageMaker endpoint, as well as models by companies like Anthropic and OpenAI. With Amazon Bedrock, you will be able to choose Amazon Titan, Amazon’s own LLM, or partner LLMs such as those from AI21 Labs and Anthropic with APIs securely without the need for your data to leave the AWS ecosystem. The additional benefits that Amazon Bedrock will offer include a serverless architecture, a single API to call the supported LLMs, and a managed service to streamline the developer workflow.\n",
    "\n",
    "For the best results, a GenAI app needs to engineer the prompt based on the user request and the specific LLM being used. Conversational AI apps also need to manage the chat history and the context. GenAI app developers can use open-source frameworks such as LangChain that provide modules to integrate with the LLM of choice, and orchestration tools for activities such as chat history management and prompt engineering. We have provided the KendraIndexRetriever class, which implements a LangChain retriever interface, which applications can use in conjunction with other LangChain interfaces such as chains to retrieve data from an Amazon Kendra index. We have also provided a few sample applications in the GitHub repo. You can deploy this solution in your AWS account using the step-by-step guide in this post."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a935567-2d68-4ba5-887e-a8e15fbad714",
   "metadata": {
    "tags": []
   },
   "source": [
    "---\n",
    "\n",
    "0. [Prerequisites](#Prerequisites)\n",
    "1. [Permissions and environment variables](#1.-Permissions-and-environment-variables)\n",
    "2. [Select a pre-trained model](#2.-Select-a-pre-trained-model)\n",
    "3. [Retrieve Artifacts & Deploy an Endpoint](#3.-Retrieve-Artifacts-&-Deploy-an-Endpoint)\n",
    "4. [Query endpoint and parse response](#4.-Query-endpoint-and-parse-response)\n",
    "5. [Query endpoint with Langchain and Kendra Index](#5.-Query-endpoint-with-Langchain-and-Kendra-Index)\n",
    "6. [[OPTIONAL] Installing Streamlet application and running a WebUI for a chatbot](#6.-[OPTIONAL]-Installing-Streamlit-application-and-running-a-WebUI-for-a-chatbot)\n",
    "7. [Clean up the endpoint](#7.-Clean-up-the-endpoint)\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05aec86a-dbd0-4dc3-8d7c-e3f53b5a98bc",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Prerequisites\n",
    "\n",
    "### Kendra Index\n",
    "\n",
    "---\n",
    "\n",
    "Use the provided AWS CloudFormation to create a new Amazon Kendra index. This template includes sample data containing AWS online documentation for Amazon Kendra, Amazon Lex, and Amazon SageMaker. Alternately, if you have an Amazon Kendra index and have indexed your own dataset, you can use that. \n",
    "\n",
    "\n",
    "Deployment steps:\n",
    "   1. Download the [template](https://github.com/senatoredu/genai-kendra-rag/blob/main/kendra-docs-index.yaml) from the github repo\n",
    "   2. Deploy it using the [cloudformation console](https://us-east-1.console.aws.amazon.com/cloudformation/home?region=us-east-1#/stacks/create)\n",
    "      1. Select Upload a template file and then the choose file button to select the template you just downloaded\n",
    "      2. Press Next\n",
    "   3. Provide a stack name and press Next   \n",
    "   4. Leave all default options and press Next\n",
    "   5. Check the acknowledgement at the bottom and press Submit\n",
    "   6. The stack will take around 15 minutes to deploy. You can leave the stack creation and proceed with the rest of the steps, when you come back proceed to Step 7\n",
    "   7. Take note of the `AWSRegion` and `KendraIndexID` in the Outputs tab as we will need it in later steps   \n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a90c94f-7689-4fe5-b9ff-636670cc43e6",
   "metadata": {},
   "source": [
    "### Kendra Permissions\n",
    "---\n",
    "\n",
    "You will need to update your SageMaker execution role with permissions to query Kendra. \n",
    "\n",
    "1. Navigate to IAM console and select Roles\n",
    "2. Search for your SageMaker execution role it will look like `sagemaker-studio-domain-SageMakerExecutionRole-<XXXXXXXXX>`\n",
    "3. Select your role\n",
    "4. Select add permission\n",
    "5. Select 'Create inline policy' and search for `AmazonKendraReadOnlyAccess` and attach the policy\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0a93461-d37f-41f4-aa1f-47bd04016865",
   "metadata": {},
   "source": [
    "# Installation and import of required dependencies, further setup tasks\n",
    "\n",
    "For this lab, we will use the following libraries:\n",
    "\n",
    " - SageMaker SDK for interacting with Amazon SageMaker\n",
    " - boto3, the AWS SDK for python\n",
    " - os, a python library implementing miscellaneous operating system interfaces \n",
    " - tarfile, a python library to read and write tar archive files\n",
    " - io, native Python library, provides Python’s main facilities for dealing with various types of I/O.\n",
    " - tqdm, a utility to easily show a smart progress meter for synchronous operations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8520390f-9909-4aba-b5e8-cc92e8c6fc21",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import sagemaker\n",
    "import boto3\n",
    "import os\n",
    "import tarfile\n",
    "import requests\n",
    "from io import BytesIO\n",
    "from tqdm import tqdm\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13f94230-975c-428c-ba4b-635f45172e8b",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Setup of notebook environment\n",
    "\n",
    "Before we begin with the actual work, we need to setup the notebook environment respectively. This includes:\n",
    "\n",
    "- retrieval of the execution role our SageMaker Studio domain is associated with for later usage\n",
    "- retrieval of our account_id for later usage\n",
    "- retrieval of the chosen region for later usage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fb17d15-59cd-4232-b42f-b15dc9ac485b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Retrieve SM execution role\n",
    "aws_role = sagemaker.get_execution_role()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bc9333b-2b74-4e0c-a5ce-4ad86ebd6cf7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Create a new STS client\n",
    "sts_client = boto3.client('sts')\n",
    "\n",
    "# Call the GetCallerIdentity operation to retrieve the account ID\n",
    "response = sts_client.get_caller_identity()\n",
    "account_id = response['Account']\n",
    "account_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "496febd5-bdd0-4d8c-9bd9-bfc24161daca",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Retrieve region\n",
    "aws_region = boto3.Session().region_name\n",
    "aws_region"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7d1f3d0-810d-4ee9-b525-3e6ed238ed36",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Setup of S3 bucket for Amazon Kendra storage of knowledge documents\n",
    "\n",
    "Amazon Kendra provides multiple built-in adapters for integrating with data sources to build up a document index, e.g. S3, web-scraper, RDS, Box, Dropbox, ... . In this lab we will store the documents containing the knowledge to be infused into the application in S3. For this purpose, (if not already present) we create a dedicated S3 bucket."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7e84b78-7453-4045-a7f9-5d1a2d42b9ec",
   "metadata": {},
   "source": [
    "## Uploading knowledge documents into an Amazon Kendra index\n",
    "\n",
    "Next we are going to add some more documents from S3 to show how easy it is to integrate different data sources to a Kendra Index. \n",
    "First we are going to download some interesting pdf files from the internet, but please feel free to drop any pdf you might find interesting in it as well. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c571ede-db5c-4ae7-b043-b9550ca4ccd7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import boto3\n",
    "import requests\n",
    "from io import BytesIO\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Create an S3 client\n",
    "s3 = boto3.client('s3')\n",
    "\n",
    "# Create a bucket if it doesn't exist\n",
    "bucket_name = f'immersion-day-bucket-{account_id}-{aws_region}'\n",
    "if s3.list_buckets()['Buckets']:\n",
    "    for bucket in s3.list_buckets()['Buckets']:\n",
    "        if bucket['Name'] == bucket_name:\n",
    "            break\n",
    "    else:\n",
    "        s3.create_bucket(Bucket=bucket_name)\n",
    "else:\n",
    "    s3.create_bucket(Bucket=bucket_name)\n",
    "\n",
    "# List of URLs to download PDFs from\n",
    "pdf_urls = [\n",
    "    \"https://s2.q4cdn.com/299287126/files/doc_financials/2023/ar/Amazon-2022-Annual-Report.pdf\",\n",
    "    \"https://s2.q4cdn.com/299287126/files/doc_financials/2022/ar/Amazon-2021-Annual-Report.pdf\",\n",
    "    \"https://s2.q4cdn.com/299287126/files/doc_financials/2021/ar/Amazon-2020-Annual-Report.pdf\",\n",
    "]\n",
    "\n",
    "# Download PDFs from the URLs and upload them to the S3 bucket\n",
    "for url in tqdm(pdf_urls):\n",
    "    response = requests.get(url, stream=True)\n",
    "    filename = os.path.basename(url)\n",
    "    print(f\"Working on {filename}\")\n",
    "    fileobj = BytesIO()\n",
    "    total_size = int(response.headers.get('content-length', 0))\n",
    "    block_size = 1024\n",
    "    progress_bar = tqdm(total=total_size, unit='iB', unit_scale=True)\n",
    "    for data in response.iter_content(block_size):\n",
    "        progress_bar.update(len(data))\n",
    "        fileobj.write(data)\n",
    "    progress_bar.close()\n",
    "    fileobj.seek(0)\n",
    "    s3.upload_fileobj(fileobj, bucket_name, filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "568dc2f6-6cba-4b4d-b600-b46997ac545b",
   "metadata": {},
   "source": [
    "Lets use those documents in Kendra. First navigate to the Kendra console. \n",
    "\n",
    "Under \"Data Management\" you will find the tab \"Data Sources\". Navigate there and add a new data source via \"Add data source\". \n",
    "Take some time to inspect all the different connectors that are there for you to use out of the box. We will use s3 as our source. \n",
    "\n",
    "It is worth noting that Kendra respect enterprise level access attributes. That means, that it can deny queries if a user is not authorized to retrieve a document. \n",
    "\n",
    "You can either add the sample bucket as a data source that has been provided on the top of the connectors, but for the sake of demonstration, we will add our downloaded pdfs as well. \n",
    "\n",
    "The animation below shows how to add an s3 data source to kendra to index. We are creating a new IAM role as well as setting the indexing frequncy to \"on-demand\". \n",
    "\n",
    "<p align=\"center\">\n",
    "  <img src=\"https://raw.githubusercontent.com/aws-samples/generative-ai-on-aws-immersion-day/4727dd546aa15eeef0440aa53a39ecf85aa49e17/img/new_s3_connection.gif\" alt=\"How to add a kendra s3 data source \"/>\n",
    "</p>\n",
    "\n",
    "\n",
    "After the connection has been established, you can sync your data source by clicking \"sync now\". "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76ba1a07-a41f-4d7a-beb7-2f8b82facd8c",
   "metadata": {},
   "source": [
    "### 1. Permissions and environment variables for SageMaker\n",
    "\n",
    "---\n",
    "To host on Amazon SageMaker, we need to set up and authenticate the use of AWS services. Here, we use the execution role associated with the current notebook as the AWS account role with SageMaker access. \n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0221f42-b4de-4c97-be49-c95915a2b945",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import sagemaker, boto3, json\n",
    "from sagemaker.session import Session\n",
    "\n",
    "sagemaker_session = Session()\n",
    "aws_role = sagemaker_session.get_caller_identity_arn()\n",
    "aws_region = boto3.Session().region_name\n",
    "sess = sagemaker.Session()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9addf0b8-479f-4f7d-b420-d53e59e6480f",
   "metadata": {
    "tags": []
   },
   "source": [
    "### 2. Select a pre-trained model\n",
    "***\n",
    "You can continue with the default model, or can choose a different model. A complete list of SageMaker pre-trained models can also be accessed at [SageMaker pre-trained Models](https://sagemaker.readthedocs.io/en/stable/doc_utils/pretrainedmodels.html#). Be sure to select a model that can be used for text2text generation.\n",
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17ed3d4d-5c77-42d3-bf1d-dccc6be203b0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model_id, model_version = \"meta-textgeneration-llama-2-7b-f\", \"*\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "102b0b66-a447-4a71-9308-8f0bd3795334",
   "metadata": {},
   "source": [
    "### 3. Retrieve Artifacts & Deploy an Endpoint\n",
    "\n",
    "***\n",
    "\n",
    "Using SageMaker, we can perform inference on the pre-trained model, even without fine-tuning it first on a new dataset. We start by retrieving the `deploy_image_uri`, `deploy_source_uri`, and `model_uri` for the pre-trained model. To host the pre-trained model, we create an instance of [`sagemaker.model.Model`](https://sagemaker.readthedocs.io/en/stable/api/inference/model.html) and deploy it. This may take a few minutes.\n",
    "\n",
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8bd9ef9-2bce-4da1-8ee1-51026ba9d52f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sagemaker.jumpstart.model import JumpStartModel\n",
    "my_model = JumpStartModel(model_id = \"meta-textgeneration-llama-2-7b-f\")\n",
    "predictor = my_model.deploy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea4c0a56-41ed-4785-bc6a-01f74a60c071",
   "metadata": {},
   "source": [
    "**If you have a predeployed endpoint you want to use you can define it here and avoid running the code block above**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05b002e0-bf24-45e5-9a56-fefa7e50d8fe",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# endpoint_name = \"<YOUR_PREDEPLOYED_ENDPOINT_NAME>\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be5c20f1-9360-4637-9649-dede0feaabc8",
   "metadata": {},
   "source": [
    "The quality of the response you will recieve is going to depend on the model you have deployed. Try deploying a different model endpoint and comparing the results you recieve from the same queries."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b21f7e4-cbf5-4b37-9648-82b6373d4df5",
   "metadata": {},
   "source": [
    "### 6. [OPTIONAL] Installing Streamlit application and running a WebUI for a chatbot\n",
    "\n",
    "---\n",
    "This sections provides instructions on how to run a streamlit application within sagemaker studio and accessing it using jupyter proxy. The commands and instructions below need to be run inside a **SageMaker System Terminal**.\n",
    "\n",
    "---\n",
    "\n",
    "1. Launch a new SageMaker System Terminal \n",
    "   1. From the SageMaker Studio Home screen select `Open Launcher`\n",
    "   2. From the Launcher panel under `Utilities and files` select `System terminal`\n",
    "2. Activate the conda environment\n",
    "```\n",
    "conda activate studio\n",
    "```\n",
    "3. Install the AWS Langchain utility classes from the repo downloaded in step 1 (make sure you're in the right folder)\n",
    "```\n",
    "pip install ./amazon-kendra-langchain-extensions\n",
    "```\n",
    "4. [Optional] For executing sample chains, install the optional dependencies\n",
    "```\n",
    "pip install \"./amazon-kendra-langchain-extensions/[samples]\"\n",
    "```\n",
    "5. Set your environment variables\n",
    "```\n",
    "export AWS_REGION=\"<YOUR_AWS_REGION>\"\n",
    "export KENDRA_INDEX_ID=\"<YOUR_KENDRA_INDEX_ID>\"\n",
    "export FLAN_XL_ENDPOINT=\"<YOUR_SAGEMAKER_ENDPOINT_FOR_FLAN_T_XL>\"\n",
    "```\n",
    "6. Run the streamlit application\n",
    "```\n",
    "cd ./amazon-kendra-langchain-extensions/samples\n",
    "streamlit run app.py flanxl\n",
    "```\n",
    "7. This will output something similar to the below, you need to take note of the port (in this case 8501)\n",
    "```\n",
    "Collecting usage statistics. To deactivate, set browser.gatherUsageStats to False.\n",
    "\n",
    "\n",
    "  You can now view your Streamlit app in your browser.\n",
    "\n",
    "  Network URL: http://169.255.255.2:8501\n",
    "  External URL: http://18.213.200.192:8501\n",
    "```\n",
    "8. Copy the current URL of the SageMaker Studio which should have the form:\n",
    "```\n",
    "https://<YOUR_STUDIO_DOMAIN>.studio.<AWS_REGION>.sagemaker.aws/jupyter/default/lab/workspaces/auto-Z/tree/kendra_rag_demo.ipynb\n",
    "```\n",
    "9. Delete everything from `lab/` onwards and replace it with `proxy/<PORT>/`\n",
    "   1. DON'T FORGET THE END `/`\n",
    "```\n",
    "https://<YOUR_STUDIO_DOMAIN>.studio.<AWS_REGION>.sagemaker.aws/jupyter/default/proxy/8501/\n",
    "```\n",
    "10. Paste the new address into the browser and you will now be able to access your chatbot UI which uses Langchain and Kendra. Each response will list the sources from Kendra it used for its answers."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26d37967-38b0-47b2-b36a-1938a1a239d3",
   "metadata": {},
   "source": [
    "### 7. Clean up the endpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3eb3d629-4778-4b72-b65e-2597ffa21805",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Delete the SageMaker endpoint\n",
    "model_predictor.delete_model()\n",
    "model_predictor.delete_endpoint()"
   ]
  }
 ],
 "metadata": {
  "availableInstances": [
   {
    "_defaultOrder": 0,
    "_isFastLaunch": true,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 4,
    "name": "ml.t3.medium",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 1,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.t3.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 2,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.t3.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 3,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.t3.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 4,
    "_isFastLaunch": true,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.m5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 5,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.m5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 6,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.m5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 7,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.m5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 8,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.m5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 9,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.m5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 10,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.m5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 11,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.m5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 12,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.m5d.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 13,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.m5d.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 14,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.m5d.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 15,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.m5d.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 16,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.m5d.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 17,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.m5d.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 18,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.m5d.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 19,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.m5d.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 20,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": true,
    "memoryGiB": 0,
    "name": "ml.geospatial.interactive",
    "supportedImageNames": [
     "sagemaker-geospatial-v1-0"
    ],
    "vcpuNum": 0
   },
   {
    "_defaultOrder": 21,
    "_isFastLaunch": true,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 4,
    "name": "ml.c5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 22,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.c5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 23,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.c5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 24,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.c5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 25,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 72,
    "name": "ml.c5.9xlarge",
    "vcpuNum": 36
   },
   {
    "_defaultOrder": 26,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 96,
    "name": "ml.c5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 27,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 144,
    "name": "ml.c5.18xlarge",
    "vcpuNum": 72
   },
   {
    "_defaultOrder": 28,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.c5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 29,
    "_isFastLaunch": true,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.g4dn.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 30,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.g4dn.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 31,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.g4dn.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 32,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.g4dn.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 33,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.g4dn.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 34,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.g4dn.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 35,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 61,
    "name": "ml.p3.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 36,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 244,
    "name": "ml.p3.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 37,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 488,
    "name": "ml.p3.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 38,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.p3dn.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 39,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.r5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 40,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.r5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 41,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.r5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 42,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.r5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 43,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.r5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 44,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.r5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 45,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 512,
    "name": "ml.r5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 46,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.r5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 47,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.g5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 48,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.g5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 49,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.g5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 50,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.g5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 51,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.g5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 52,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.g5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 53,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.g5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 54,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.g5.48xlarge",
    "vcpuNum": 192
   },
   {
    "_defaultOrder": 55,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 1152,
    "name": "ml.p4d.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 56,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 1152,
    "name": "ml.p4de.24xlarge",
    "vcpuNum": 96
   }
  ],
  "instance_type": "ml.t3.medium",
  "kernelspec": {
   "display_name": "Python 3 (Data Science 3.0)",
   "language": "python",
   "name": "python3__SAGEMAKER_INTERNAL__arn:aws:sagemaker:us-east-1:081325390199:image/sagemaker-data-science-310-v1"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
